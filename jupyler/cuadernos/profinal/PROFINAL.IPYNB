{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f739c4c",
   "metadata": {},
   "source": [
    "# Análisis Estadístico Multivariado\n",
    "# 29 de Noviembre de 2025\n",
    "# Proyecto final\n",
    "# Acoyani Garrido Sandoval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376105c6",
   "metadata": {},
   "source": [
    "## 1. Descripción\n",
    "\n",
    "Consiste en determinar la relación entre una variable dependiente y una o más variables independientes a través de regresión logística múltiple. Se usará para ello un conjunto de datos de patrones de órdenes de comida rápida en diferentes ciudades de India, desarrollado por Prince Rajak y disponible en [Kaggle](https://www.kaggle.com/datasets/prince7489/fast-food-ordering-pattern-dataset).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef045f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "import matplotlib.pyplot as pyplot\n",
    "import json\n",
    "import seaborn\n",
    "import datetime\n",
    "\n",
    "# Boilerplate: necesitamos un JSON encoder capaz de manejar números de Numpy\n",
    "class SerializadorJSONParaLaClaseDeTemores(json.JSONEncoder):\n",
    "   def default(self, objeto):\n",
    "      # numpy.int64: lo convertimos a un número estándar de Python para que el serializador estándar\n",
    "      # pueda manejarlo\n",
    "      if isinstance(objeto, numpy.int64) or isinstance(objeto, numpy.int32):\n",
    "         return objeto.item()\n",
    "      # Cualquier otro tipo: usamos el serializador estándar\n",
    "      else:\n",
    "         return json.JSONEncoder.default(self, objeto)\n",
    "\n",
    "\n",
    "# Tomamos la primera columna como índice, porque es un número de ID\n",
    "df_pedidos = pandas.read_csv(\"fast_food_ordering_dataset.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4f6f22",
   "metadata": {},
   "source": [
    "## 2. Análisis descriptivo\n",
    "\n",
    "Comenzamos por desarrollar un análisis descriptivo de las variables del conjunto de datos.\n",
    "\n",
    "Cada registro de nuestro conjunto de datos es un pedido individual de comida rápida, cuyas variables son:\n",
    "- **order_id:** Número de pedido\n",
    "- **order_time:** Fecha y hora del pedido en tiempo de India\n",
    "- **city:** Ciudad donde fue hecho el pedido\n",
    "- **cuisine_type:** Estilo de cocina\n",
    "- **order_value:** Precio del pedido en rupias\n",
    "- **delivery_time_minutes:** Tiempo del pedido en minutos\n",
    "- **items_count:** Cuántos elementos tiene el pedido\n",
    "- **payment_method:** Método de pago:\n",
    "   - **UPI:** *Unified Payments Interface,* un sistema de pagos electrónicos operado por el banco nacional de India\n",
    "   - **Wallet:** pagos hechos mediante sistemas orientados a dispositivos móviles, tales como Apple y Google Pay.\n",
    "   - **Credit Card:** tarjeta de crédito\n",
    "   - **Debit Card:** tarjeta de débito\n",
    "   - **Cash:** efectivo\n",
    "\n",
    "El análisis que haremos incluirá:\n",
    "\n",
    "1. **Para las variables de razón:** Medidas de tendencia central y de dispersión: moda, media, mediana, rango, desviación estándar, rango intercuartílico, y diagrama de caja.\n",
    "   - Minutos de entrega\n",
    "   - Cantidad de elementos\n",
    "   - Precio\n",
    "1. **Para la variable de intervalo:** Distribución (histograma), moda y mediana\n",
    "   - Hora del pedido\n",
    "1. **Para las variables nominales:** Conteo en gráfica, y porcentaje\n",
    "   - Ciudad\n",
    "   - Estilo de cocina\n",
    "   - Método de pago\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3ac62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parte 1: medidas de las variables de razón\n",
    "variables_razon = [ \"delivery_time_minutes\", \"items_count\", \"order_value\" ]\n",
    "variables_intervalo = [ \"order_time\" ]\n",
    "variables_nominales = [ \"city\", \"cuisine_type\", \"payment_method\" ]\n",
    "\n",
    "# Medidas de las variables de razón\n",
    "variables_razon_medidas = {}\n",
    "for una_variable in variables_razon:\n",
    "   medidas = \\\n",
    "   {\n",
    "      \"promedio\": df_pedidos[una_variable].mean(),\n",
    "      \"mediana\": df_pedidos[una_variable].median(),\n",
    "      \"moda\": df_pedidos[una_variable].mode()[0],\n",
    "      \"varianza\": df_pedidos[una_variable].var(),\n",
    "      \"desvstd\": df_pedidos[una_variable].std(),\n",
    "      \"rango\": df_pedidos[una_variable].max() - df_pedidos[una_variable].min(),\n",
    "      \"RI\": df_pedidos[una_variable].quantile(0.75) - df_pedidos[una_variable].quantile(0.25)\n",
    "   }\n",
    "   variables_razon_medidas.update({ una_variable: medidas })\n",
    "\n",
    "print(f\"Medidas de las variables de razón: {json.dumps(variables_razon_medidas, indent=3, cls=SerializadorJSONParaLaClaseDeTemores)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259573d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora grafico los histogramas\n",
    "for una_variable in variables_razon:\n",
    "   pyplot.figure()\n",
    "   pyplot.hist(df_pedidos[una_variable])\n",
    "   pyplot.xlabel(\"Valores\")\n",
    "   pyplot.ylabel(\"Frecuencia\")\n",
    "   pyplot.title(f\"{una_variable}: histograma\")\n",
    "   pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed54cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parte 2: medidas de la variable de intervalo: moda y mediana\n",
    "variables_intervalo_medidas = {}\n",
    "\n",
    "# Convertimos order_time a datetime.datetime y extraemos la hora\n",
    "# Tenemos que dejarla en forma de una hora del día\n",
    "df_pedidos[\"order_time\"] = pandas.to_datetime(df_pedidos[\"order_time\"])\n",
    "df_pedidos[\"order_time\"] = df_pedidos[\"order_time\"].dt.hour\n",
    "\n",
    "# Sacamos moda y mediana\n",
    "hora_moda = order_time_datetime.mode()[0]\n",
    "hora_mediana = order_time_datetime.median()\n",
    "\n",
    "# Sacamos un histograma de pedidos por hora\n",
    "pyplot.figure(figsize=(10, 6))\n",
    "pyplot.hist(order_time_datetime, bins=24, edgecolor='black')\n",
    "pyplot.xlabel('Hora del día')\n",
    "pyplot.ylabel('Frecuencia')\n",
    "pyplot.title('Distribución de órdenes por hora')\n",
    "pyplot.xticks(range(0, 24))\n",
    "pyplot.grid(axis='y', alpha=0.3)\n",
    "pyplot.show()\n",
    "\n",
    "# Guardamos nuestras medidas\n",
    "variables_intervalo_medidas[\"order_time\"] = {\n",
    "   \"moda\": hora_moda,\n",
    "   \"mediana\": hora_mediana\n",
    "}\n",
    "\n",
    "print(f\"Medidas de la variable de intervalo: {json.dumps(variables_intervalo_medidas, indent=3, cls=SerializadorJSONParaLaClaseDeTemores)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f93ce34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parte 3: medidas de las variables nominales\n",
    "# Sacamos conteo y porcentaje\n",
    "variables_nominales_medidas = {}\n",
    "\n",
    "for una_variable in variables_nominales:\n",
    "   medidas = \\\n",
    "   {\n",
    "      \"conteos\": {},\n",
    "      \"porcentajes\": {}\n",
    "   }\n",
    "\n",
    "   # En cada variable, sacamos los datos únicos\n",
    "   valores_nominales = list(numpy.unique(numpy.array(df_pedidos[una_variable])))\n",
    "   \n",
    "   # Luego sacamos cuánto hay de cada valor único\n",
    "   # De una vez sacamos el total\n",
    "   sumatoria = 0\n",
    "   for un_valor in valores_nominales:\n",
    "      columna = df_pedidos[una_variable]\n",
    "      columna_filtrada = columna[ columna == un_valor ]\n",
    "      medidas[\"conteos\"].update({ un_valor: len(columna_filtrada)})\n",
    "      sumatoria += len(columna_filtrada)\n",
    "   \n",
    "   # Luego sacamos los porcentajes (del 0 al 1)\n",
    "   for clave, valor in medidas[\"conteos\"].items():\n",
    "      medidas[\"porcentajes\"].update({ clave: float(valor) / sumatoria })\n",
    "\n",
    "   # Revisamos el resultado y lo incorporamos\n",
    "   variables_nominales_medidas.update({ una_variable: medidas })\n",
    "\n",
    "print(f\"Medidas de las variables nominales: {json.dumps(variables_nominales_medidas, indent=3, cls=SerializadorJSONParaLaClaseDeTemores)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165b471e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficamos el paso anterior\n",
    "for una_variable in variables_nominales:\n",
    "   medidas = variables_nominales_medidas[una_variable]\n",
    "   conteos = [ valor for _, valor in medidas[\"conteos\"].items() ]\n",
    "   porcentajes = [ porcentaje * 100 for _, porcentaje in medidas[\"porcentajes\"].items() ]\n",
    "   valores = [ clave for clave, _ in medidas[\"conteos\"].items() ]\n",
    "\n",
    "   # Creamos una gráfica de 14 x 5 \"pulgadas\"\n",
    "   figura, ejes = pyplot.subplots(1, 2, figsize=(14,5))\n",
    "   figura.suptitle(f\"{una_variable}: conteos y porcentajes\")\n",
    "\n",
    "   eje_0 = ejes[0]\n",
    "   seaborn.barplot(x=conteos, y=valores, palette=\"viridis\", ax=eje_0)\n",
    "   eje_0.set_title(\"Conteos\")\n",
    "\n",
    "   eje_1 = ejes[1]\n",
    "   seaborn.barplot(x=porcentajes, y=valores, palette=\"magma\", ax=eje_1)\n",
    "   eje_1.set_title(\"Porcentajes\")\n",
    "\n",
    "   # Pyplot por sí mismo no acomoda bien las dimensiones de las etiquetas\n",
    "   # Para eso, usamos tight_layout. El parámetro rect indica cuánto debe abarcar el contenido de la\n",
    "   # gráfica.\n",
    "   pyplot.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "   pyplot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390260bf",
   "metadata": {},
   "source": [
    "## 5 y 3. Partición del conjunto de datos y selección de variables con método de regularización\n",
    "\n",
    "Consiste en realizar selección de variables con el método de regularización L1 (Lasso) previo a la regresión logística.\n",
    "\n",
    "Para llevar a cabo este proceso, hay que normalizar las escalas de los datos, ya que la técnica LASSO se basa en la manipulación de los coeficientes de la regresión logística, los cuales siguen la escala de las variables que representan.\n",
    "\n",
    "En este paso también realizamos la partición del conjunto de datos en entrenamiento y prueba. Usamos para eso muestreo aleatorio uniforme, tomamos 80% del conjunto para entrenamiento, y dejamos 20% para prueba.\n",
    "\n",
    "Como decisión de negocio, determinamos hacer nuestro modelo bajo el escenario de implementar en una plataforma de comida rápida a domicilio una función para presentar al usuario final un tiempo de entrega estimado; por lo que tomamos nuestro tiempo de entrega como variable dependiente. Será necesario también descartar las variables categóricas, pues la regresión logística sólo funciona con variables numéricas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6308cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Antes de proceder, partimos nuestros datos. Para eso, train_test_split admite dataframes\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "# El ejemplo del Jupyler notebook usa un dataset que ha sido ampliamente adoptado para practicar\n",
    "# la regresión logística, ya que tiene 30 variables independientes y una variable independiente\n",
    "# binaria.\n",
    "# Aquí tenemos menos variables, y nuestra variable dependiente, que es el tiempo de entrega, es\n",
    "# numérica y discreta. Adoptamos la convención de llamar \"y\" a nuestra variable dependiente, y \"X\"\n",
    "# a nuestras variables independientes.\n",
    "df_pedidos_sincategoricas = df_pedidos.copy()\n",
    "y_tiempoentrega = df_pedidos_sincategoricas.pop(\"delivery_time_minutes\")\n",
    "\n",
    "# La regresión logística sólo funciona con variables numéricas, por lo que hay que botar las \n",
    "# categóricas\n",
    "df_pedidos_sincategoricas.pop(\"city\")\n",
    "df_pedidos_sincategoricas.pop(\"cuisine_type\")\n",
    "df_pedidos_sincategoricas.pop(\"payment_method\")\n",
    "\n",
    "# Particionamos nuestro conjunto\n",
    "X_entrenamiento, X_prueba, y_entrenamiento, y_prueba = train_test_split(df_pedidos_sincategoricas, y_tiempoentrega, test_size=0.2, random_state=297974)\n",
    "\n",
    "# Escalamos los datos\n",
    "escalador = StandardScaler()\n",
    "X_entrenamiento_escalao = escalador.fit_transform(X_entrenamiento)\n",
    "X_prueba_escalao = escalador.transform(X_prueba)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334f082e",
   "metadata": {},
   "source": [
    "## 4. Acerca del `penalty`\n",
    "\n",
    "**Explica brevemente qué es el penalty, qué tipos de penalty hay para LASSO y sus diferencias:**\n",
    "\n",
    "El *penalty* en LASSO es un término de regularización para la función objetivo de la regresión logística. Una forma de hacerlo es con el llamado *L1 penalty,* el cual es simplemente la sumatoria de los valores absolutos de los coeficientes beta multiplicado por un *parámetro de regularización* que controla la fuerza que tiene ese proceso de regularización.\n",
    "\n",
    "El efecto que el *penalty* tiene en los modelos de regresión logística es reducir coeficientes a cero; entre mayor sea su fuerza, más coeficientes se reducen. Esto permite simplificar el modelo a través de eliminar características posiblemente irrelevantes de forma analítica. Es también por eso que es necesario normalizar la escala de las variables involucradas; de lo contrario, aplicar un parámetro de regularización para pedidos de miles de rupias a un juego de datos con variables que no pasan de 60 (minutos) resultaría en la cancelación de todos los coeficientes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22299a27",
   "metadata": {},
   "source": [
    "## 6. Entrenamiento de un modelo de regresión logística múltiple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aacc190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora sí, entrenamos nuestro modelo con regularización L1 (LASSO)\n",
    "modelo = LogisticRegression(penalty=\"l1\", solver=\"saga\", max_iter=10000)\n",
    "modelo.fit(X_entrenamiento_escalao, y_entrenamiento)\n",
    "\n",
    "# Entrenado nuestro modelo, obtenemos las características más relevantes\n",
    "coeficientes = modelo.coef_\n",
    "caracteristicas_seleccionadas = numpy.where(coeficientes != 0)[1]\n",
    "print(f\"Características seleccionadas: {caracteristicas_seleccionadas}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963e56ef",
   "metadata": {},
   "source": [
    "## 7. Probar el modelo y obtener métricas relevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9ea47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora filtramos nuestro conjunto de datos con las características seleccionadas\n",
    "X_entrenamiento_escalao_filtrao = X_entrenamiento_escalao[:, caracteristicas_seleccionadas]\n",
    "X_prueba_escalao_filtrao = X_prueba_escalao[:, caracteristicas_seleccionadas]\n",
    "\n",
    "# Antes de probar, sacamos otro modelo a partir de los datos filtrados\n",
    "# Ya no usamos penalty, porque ya lo aplicamos en el modelo anterior\n",
    "modelo_filtrao = LogisticRegression(solver=\"saga\", max_iter=10000)\n",
    "modelo_filtrao.fit(X_entrenamiento_escalao_filtrao, y_entrenamiento)\n",
    "\n",
    "# Y ahora probamos el segundo modelo que sacamos a partir de las características filtradas\n",
    "puntuacion = modelo_filtrao.score(X_prueba_escalao_filtrao, y_prueba)\n",
    "print(f\"La precisión del modelo con las características elegidas es: {puntuacion * 100}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e171115",
   "metadata": {},
   "source": [
    "## 8. ¿Es necesario corregir desbalances de clases?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f8d45c",
   "metadata": {},
   "source": [
    "## 9. Interpretación de los resultados obtenidos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd6f086",
   "metadata": {},
   "source": [
    "## 10. Conclusiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c2900a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e3e1dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyler",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
